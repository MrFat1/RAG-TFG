{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=rIV1EseKwU4&ab_channel=AIAnytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n",
    "!pip install -q torch transformers langchain sentence_transformers\n",
    "!pip install -q accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  pipeline\n",
    ")\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638269f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cea834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Base inference\n",
    "#################################################################\n",
    "prompt = tokenizer.encode_plus(\"[INST] Tell me about fantasy football? [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "\n",
    "generated_ids = model.generate(prompt, \n",
    "                               max_new_tokens=1000, \n",
    "                               do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b402052",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, PDFMinerLoader\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "\n",
    "# Will load all the documents avaible in the path\n",
    "def load_documents():\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith(\"*.pdf\"):\n",
    "                print(file)\n",
    "                loader = PDFMinerLoader(os.path.join(root, file))\n",
    "    \n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Cogemos un chunk aleatorio\n",
    "    document = chunks[10]\n",
    "    print(document.page_content) # Printeamos su contenido\n",
    "    print(document.metadata) # Y su metadata (Fichero al que pertenece y donde empieza 'start_index')\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a18408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "import shutil\n",
    "\n",
    "def save_to_chroma(chunks):\n",
    "    \n",
    "    # Clear out the database first if already exists.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    #embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    \n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, \n",
    "        embeddings, \n",
    "        persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    \n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_model():\n",
    "    text_generation_pipeline = transformers.pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=True,\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3624ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def query_data():\n",
    "    llm = load_model()\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def query_data2():\n",
    "    llm = load_model()\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    ### [INST] \n",
    "    Instruction: Answer the question based on your \n",
    "    fantasy football knowledge. Here is context to help:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    ### QUESTION:\n",
    "    {question} \n",
    "\n",
    "    [/INST]\n",
    "     \"\"\"\n",
    "    \n",
    "    promptTemplate = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    llm_chain = LLMChain(llm=llm, prompt=promptTemplate)\n",
    "\n",
    "    # RunnablePassthrough para pasar la query al siguiente step en la chain\n",
    "    rag_chain = ( \n",
    "     {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | llm_chain\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] Instrucción: Eres un experto en inteligecia artificial responda en español la pregunta según sus conocimientos de la siguiente página web. Aquí hay contexto para ayudar:\n",
    "\n",
    "{context}\n",
    "\n",
    "### PREGUNTA:\n",
    "{question} (responde en castellano) [/INST]\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53905675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query):\n",
    "    qa = query_data()\n",
    "    generated_text = qa(query)\n",
    "    answer = generated_text['result']\n",
    "    \n",
    "    return answer, generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c957c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer2(query):\n",
    "    \n",
    "    rag_chain = query_data2()\n",
    "    answer = rag_chain.invoke(query)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['context']\n",
    "answer['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
